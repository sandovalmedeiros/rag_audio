# ğŸ§ RAG sobre Arquivos de Ãudio com AssemblyAI, LLMs e Qdrant

Este projeto implementa um sistema RAG (Retrieval-Augmented Generation) que permite fazer perguntas
sobre o conteÃºdo de arquivos de Ã¡udio transcritos, combinando **transcriÃ§Ã£o com IA**,
**armazenamento vetorial** e **modelos de linguagem natural**.

---

## ğŸ” Tecnologias Utilizadas

- **AssemblyAI** â€“ TranscriÃ§Ã£o automÃ¡tica de Ã¡udio com API de ponta
- **LlamaIndex** â€“ Estrutura RAG e interface com LLMs
- **Qdrant VectorDB** â€“ Banco vetorial de alta performance
- **Streamlit** â€“ Interface web interativa
- **SambaNova / Ollama** â€“ Suporte dinÃ¢mico a LLMs locais e em nuvem

---

## ğŸ“¦ InstalaÃ§Ã£o

### 1. Clone o repositÃ³rio

```bash
git clone https://github.com/sandovalmedeiros/rag_audio.git
cd rag_audio
```

### 2. Crie e ative o ambiente virtual

```bash
python -m venv .venv
.venv\Scripts\activate  # Windows
```

### 3. Instale as dependÃªncias

```bash
pip install -r requirements.txt
```

### 4. Configure o arquivo `.env`

Copie o arquivo de exemplo:

```bash
cp .env.example .env
```

Edite com suas chaves reais:

Configurar o AssemblyAI:

Obtenha uma chave de API do AssemblyAI (http://bit.ly/4bGBdux) e defina-a no arquivo .env da seguinte forma:

ASSEMBLYAI_API_KEY=<SUA_CHAVE_API>

Configurar o SambaNova:

Obtenha uma chave de API do SambaNova (https://sambanova.ai/) e defina-a no arquivo .env da seguinte forma:

SAMBANOVA_API_KEY=<SUA_CHAVE_API_SAMBANOVA>
ObservaÃ§Ã£o: em vez do SambaNova, vocÃª tambÃ©m pode usar o Ollama.

```env
ASSEMBLYAI_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
SAMBANOVA_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
LLM_MODEL_NAME=DeepSeek-R1-Distill-Llama-70B  # ou outro modelo como mistral ou tinyllama via Ollama
```

---

## âš™ï¸ ExecuÃ§Ã£o simplificada (Windows)

### â–¶ï¸ Iniciar o Qdrant (banco vetorial)

```bash
run_docker.bat
```

> Esse comando inicia o Qdrant via Docker. NecessÃ¡rio apenas uma vez por sessÃ£o.

### â–¶ï¸ Iniciar a aplicaÃ§Ã£o web

```bash
run_app.bat
```

> Esse comando ativa o ambiente virtual e inicia a interface web no navegador.

---

## ğŸ§  Modelos Suportados

VocÃª pode alternar dinamicamente entre os seguintes backends no `.env`:

| Provedor     | Exemplo de valor em `LLM_MODEL_NAME`          |
|--------------|-----------------------------------------------|
| SambaNova    | `DeepSeek-R1-Distill-Llama-70B`               |
| Ollama local | `mistral`, `llama2`, `tinyllama`, `llama3.1:8b`|

---

## ğŸ³ Qdrant via Docker (alternativa manual)

```bash
docker-compose up -d
```

> Certifique-se de que as portas `6333` e `6334` estÃ£o liberadas.

---

## ğŸš€ Executando a aplicaÃ§Ã£o (modo tÃ©cnico)

```bash
streamlit run app.py
```

---

## ğŸ“ Estrutura dos Arquivos

- `app.py` â€“ Interface com Streamlit
- `rag_code.py` â€“ LÃ³gica de vetorizaÃ§Ã£o, RAG e consulta ao LLM
- `recreate_collection.py` â€“ Script para resetar a coleÃ§Ã£o do Qdrant
- `.env` â€“ Chaves da API e escolha do modelo
- `assets/` â€“ Logos utilizados na interface
- `run_app.bat` â€“ Script auxiliar para execuÃ§Ã£o no Windows
- `run_docker.bat` â€“ Script auxiliar para iniciar o banco vetorial

---

## ğŸ› ï¸ Requisitos

- Python 3.11+
- Docker (para Qdrant)
- RAM recomendada: â‰¥8 GB para modelos locais

---

## ğŸ’¡ Dica

Se vocÃª tiver pouca memÃ³ria RAM, use modelos como:

```env
LLM_MODEL_NAME=tinyllama
```

E evite modelos como `llama3.1:8b` localmente.

---

## ğŸ¤ CrÃ©ditos

Adaptado de: https://github.com/patchy631/ai-engineering-hub/tree/main/chat-with-audios?ref=dailydoseofds.com
Com â¤ï¸ por Sandova.
Inspirado por soluÃ§Ãµes modernas de RAG com Ã¡udio e LLMs.
